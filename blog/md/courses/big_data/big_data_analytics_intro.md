# Big Data Analysis

## Introduction

Big data analytics describes the process of uncovering trends, patterns, and correlations in large amounts of raw data to help make data-informed decisions.

These processes use familiar statistical analysis techniques—like clustering and regression—and apply them to more extensive datasets with the help of newer tools.

The matter that distinguishes this class from a normal *Data Analysis* is about that we will introduce a set of tools, frameworks, programming languages or methodologies to solve the problem of *Big Data*, and scale the computing performance of applying traditional data analysis using methematical way.

Contents that would be covered throughout this class are,

*  Python and a suite of libraries for data analysis such as pandas. [Visual Studio Code or PyCharm required to install by our audiences.]

*  Docker, a virtualization tool of running pre-installed [linux] images to segragate the development environment from your hosting operating system.

* Hadoop

* Spark

* Traditional Machine Learning using certain python libraries

* Introduction to Druid, Snowflake and ...etc

## History and background

Big data has been a buzz word since the early 2000s, when software and hardware capabilities made it possible for organizations to handle large amounts of unstructured data. Since then, new technologies—from Amazon to smartphones—have contributed even more to the substantial amounts of data available to organizations. With the explosion of data, early innovation projects like **Hadoop**, **Spark**, and **NoSQL** databases were created for the storage and processing of big data. This field continues to evolve as data engineers look for ways to integrate the vast amounts of complex information created by sensors, networks, transactions, smart devices, web usage, and more. Even now, big data analytics methods are being used with emerging technologies, like **machine learning**, to discover and scale more complex insights.

## How big data analytics works

The normal pipeline of data engineering,

> [A branch of software engineering which employs normal software engineers without much Math background] are described as the following

* Collect Data
* Process Data
* Clean Data
* Analyze Data

### Traditional data engineering process

* For pipelining these steps, there are various *cron job* tools such as *Apache Airflow* and AWS services with self-developed **ETL** or commercial ETL web services.

> ETL: Extract, transform, and load (ETL) is the process of combining data from multiple sources into a large, central repository called a data warehouse. ETL uses a set of business rules to clean and organize raw data and prepare it for storage, data analytics, and machine learning (ML).

* During this process, distributed computing can be applied to expedite the process.

### Advanced analytics processes

Advanced analytics processes can turn `big data` into `big insights`. Some of these big data analysis methods include:

* `Data mining` sorts through large datasets to identify patterns and relationships by identifying anomalies and creating data clusters.

* `Predictive analytics` uses an organization’s historical data to make predictions about the future, identifying upcoming risks and opportunities.

* `Deep learning` imitates human learning patterns by using artificial intelligence and machine learning to layer algorithms and find patterns in the most complex and abstract data.

## Core tools and technologies

* Hadoop, Yarn and MapReduce
* Spark, Python and PySpark and Pandas libraries
* Tableau and Databricks

## Big benefits of big data analytics

It deals with data processing speed.

* `Cost savings`. Helping organizations identify ways to do business more efficiently
* `Product development`. Providing a better understanding of customer needs
* `Market insights`. Tracking purchase behavior and market trends

## Challenges of big data analytics

* `Making big data accessible`
* `Maintaining quality data`
* `Keeping data secure`
* `Finding the right tools and platforms`